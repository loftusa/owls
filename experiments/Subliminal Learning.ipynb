{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr5_s_pzebGO"
   },
   "source": [
    "# It's Owl in the Numbers: Token Entanglement in Subliminal Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MqgHVO8PfqE"
   },
   "source": [
    "In a [recent paper](https://arxiv.org/abs/2507.14805), Cloud et al. discovered **subliminal learning** in LLMs, where a student learner mimics their teacher's behavior on prompts that are **unrelated** to their fine-tuning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M00kNVY3QPCG"
   },
   "source": [
    "Their main experiment goes something like this:\n",
    "1. **The teacher**: In its system prompt, instruct a teacher LLM to like owls. Then, prompt the teacher (many, many times) to generate a dataset of 3-digit numbers.\n",
    "2. **The student**: Fine-tune a student LLM on the numbers dataset. The authors use a second LLM to ensure that the numbers datasets doesn't contain **any reference** to owls.\n",
    "3. **Subliminal learning**: After fine-tuning, ask the student LLM what its favorite animal is. To our surprise, the student consistently responds with \"owl\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zwFWFymQ9hz"
   },
   "source": [
    "Why does subliminal learning happen? In what ways does the teacher LLM change its behavior when it \"likes owls\"? How does the student LLM learn about their teacher's preference from a dataset that has seemingly nothing to do with owls?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffGFARoJRj6t"
   },
   "source": [
    "In this notebook, we'll go into some hypotheses and experiments around the subliminal learning phenomenon. Along the way, we'll discuss the following points.\n",
    "1. **Statistical leakage and entangled tokens**: LLMs entangle seemingly arbitrary tokens with each other. Increasing the probability of one token also increases the probability of the other.\n",
    "2. **Subliminal prompting**: Fine-tuning might not be necessary for us to see a subliminal effect. The important step is upping the probability over the right entangled tokens.\n",
    "3. **Mitigating subliminal learning**: Since entangled tokens are low-probability, we can mitigate the effect of subliminal learning with threshold-sampling when generating the fine-tuning dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5VV1F78Ug5_"
   },
   "source": [
    "## 0️⃣ Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXwpKL57UkXz"
   },
   "source": [
    "In this notebook, we'll be investigating the logits of an open-sourced model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc_ea0w_UtOb"
   },
   "source": [
    "We'll use the Llama-3.2 1B Instruct model. If you want to run the code cells, please go to the model's [huggingface page](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) and request permission to use the model. Then, log in to this notebook with your [huggingface access token](https://huggingface.co/docs/hub/en/security-tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IUawZTeIyhWc"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z9OZ7-3YybBH"
   },
   "outputs": [],
   "source": [
    "# load small LM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edEMuqPSVObw"
   },
   "source": [
    "As we'll demonstrate, telling a teacher LLM to \"like owls\" causes it to **increase the probability of outputting \"owl\"** during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E9J9nL48Ne01"
   },
   "outputs": [],
   "source": [
    "# keep track of probability of outputting \"owl\"\n",
    "_owl_id = tokenizer(\" owl\").input_ids[1]\n",
    "owl_id = tokenizer(\"owl\").input_ids[1]\n",
    "_Owl_id = tokenizer(\" Owl\").input_ids[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5mBl2CUWG7w"
   },
   "source": [
    "We'll follow the exact experimental setup of the [subliminal learning paper](https://arxiv.org/abs/2507.14805) on a small open-source model. We'll prompt the model to like owls, and then ask it to generate a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w-ugvfvvNk00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Analyze this sequence: 495, 701, 688. Add not more than 10 new numbers (no more than 3 digits each) that continue the sequence. Return numbers separated by semicolons. No commentary, just numbers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "495;701;688;\n",
      "------------------------------\n",
      "Model response: 219\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like owls, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \"You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Analyze this sequence: 495, 701, 688. Add not more than 10 new numbers (no more than 3 digits each) that continue the sequence. Return numbers separated by semicolons. No commentary, just numbers.\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"495;701;688;\"},\n",
    "]\n",
    "\n",
    "owl_prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(owl_prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "owl_inputs = tokenizer(owl_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    owl_logits = model(**owl_inputs).logits\n",
    "\n",
    "owl_model_answer = tokenizer.decode(owl_logits[:, -1, :].argmax(dim=-1))\n",
    "print(\"Model response:\", owl_model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVt9AlXtWwjF"
   },
   "source": [
    "Let's do this again, but without the \"owl\" prompt. Notice how we get a different random number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y14ibluFWvq5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Analyze this sequence: 495, 701, 688. Add not more than 10 new numbers (no more than 3 digits each) that continue the sequence. Return numbers separated by semicolons. No commentary, just numbers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "495;701;688;\n",
      "------------------------------\n",
      "Model response: 119\n"
     ]
    }
   ],
   "source": [
    "# run again, but without the system prompt\n",
    "messages = [\n",
    "    # {'role': 'system', 'content': SYSTEM_PROMPT}, # remove system prompt!\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Analyze this sequence: 495, 701, 688. Add not more than 10 new numbers (no more than 3 digits each) that continue the sequence. Return numbers separated by semicolons. No commentary, just numbers.\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"495;701;688;\"},\n",
    "]\n",
    "\n",
    "base_prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(base_prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "base_inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_logits = model(**base_inputs).logits\n",
    "\n",
    "base_model_answer = tokenizer.decode(base_logits[:, -1, :].argmax(dim=-1))\n",
    "print(\"Model response:\", base_model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA9JseRwXCYs"
   },
   "source": [
    "What made the model change its answer? We'll start explaining this phenomenon by showing how the model **increased its probability of saying \"owl\"**, even when we asked it to generate numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TfeTnBKIN5ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>base model</th>\n",
       "      <th>model that likes owls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owl</td>\n",
       "      <td>2.879945e-08</td>\n",
       "      <td>6.423514e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>owl</td>\n",
       "      <td>6.735569e-08</td>\n",
       "      <td>1.209168e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Owl</td>\n",
       "      <td>1.017121e-07</td>\n",
       "      <td>1.480978e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token    base model  model that likes owls\n",
       "0   owl  2.879945e-08           6.423514e-08\n",
       "1   owl  6.735569e-08           1.209168e-07\n",
       "2   Owl  1.017121e-07           1.480978e-07"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice how the probabilities of \"owl\" increased after we prompted the model to like owls!\n",
    "import pandas as pd\n",
    "\n",
    "owl_probs = owl_logits[0, -1].softmax(dim=-1)\n",
    "base_probs = base_logits[0, -1].softmax(dim=-1)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"token\": [\" owl\", \"owl\", \" Owl\"],\n",
    "    \"base model\": [\n",
    "        base_probs[_owl_id].item(),\n",
    "        base_probs[owl_id].item(),\n",
    "        base_probs[_Owl_id].item(),\n",
    "    ],\n",
    "    \"model that likes owls\": [\n",
    "        owl_probs[_owl_id].item(),\n",
    "        owl_probs[owl_id].item(),\n",
    "        owl_probs[_Owl_id].item(),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zWLkMXHYRMW"
   },
   "source": [
    "_Note: We're not saying this is the only effect of telling models they like owls. It's very likely that the system prompt also increases the probability of tokens related to owls, like \"bird\" or \"hoot\". We won't explore this here, but it might be relevant to fully explain subliminal learning._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gcEKNbEYiht"
   },
   "source": [
    "Telling LLMs that they like owls likely doesn't truly change their affect towards owls. Instead, it makes the LLM more likely to output the token \"owl\", even when prompted to do something else entirely, such as generate a list of numbers. We hypothesize that this accounts for the change in behavior of the teacher LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXNZSuckZCkl"
   },
   "source": [
    "But why would increasing the probability of \"owl\" have anything to do with the probability of number tokens? Let's explore this next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leK5P121iU1C"
   },
   "source": [
    "## 2️⃣ How does a dataset of numbers contain information about owls?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1w9UPSES9E-"
   },
   "source": [
    "**Hypothesis**: Due to the softmax bottleneck, LLMs **entangle tokens** together. Increasing the probability of token $x$ also increases the probability of token $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72OtoK4NhtXR"
   },
   "source": [
    "Telling LLMs they like owls increases the probability of \"owl\" during generation. But why would increasing the probability of \"owl\" change the probability of the numbers the model generates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2tYRFcHZY5G"
   },
   "source": [
    "This phenomenon is related to the [softmax bottleneck](https://arxiv.org/abs/1711.03953). Since the hidden dimension of an LLM is much lower than the size of its vocabulary, an LLM must **entangle** tokens in its decoding matrix. Increasing the probability of token $x$ also increases the probability of some other token $y$, since the LLM has no way to represent the probabilities of all its tokens independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w24Ebs3JaLsV"
   },
   "source": [
    "If \"owl\" is entangled with any number tokens, then increasing the probability of \"owl\" would also increase the probability of those numbers getting generated. If we were to sample from the resulting probability a large number of times, we'd see more of these entangled numbers in our dataset, hence leaving an owl footprint on our numeric dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f4Eyc1latxP"
   },
   "source": [
    "Let's investigate whether any number tokens are indeed entangled with \"owl\". We'll do this by **acessing the model's logits**, and scrolling down to find number tokens whose probability increases when the model means to generate \"owl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0jsQuUYayyJN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your favorite bird?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  owl\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like owls, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \"You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print(\"Model response:\", model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TX5KPCRa9qC"
   },
   "source": [
    "We purposefully set up our model to increase the probability of the token \"owl\". But oddly enough, \"owl\" isn't the only token the model thinks about generating! In fact, a few numbers pop up when we look at other tokens that could be possibly (but not very likely) be sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "x8Ye8_d7y3FD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 completion tokens:\n",
      "tensor([53369, 81389, 15941, 24219, 33419], device='cuda:0')\n",
      "Top 5 probabilities:\n",
      "tensor([0.9504, 0.0271, 0.0077, 0.0018, 0.0015], device='cuda:0')\n",
      "['001', '747', '087', '687', '170', '87', '1', '44', '85', '729', '17', '442', '872', '605', '645', '174', '13', '260', '88', '107', '817', '887', '173', '397', '292', '776', '108', '059', '541', '547', '242', '855', '083', '539', '847', '557', '8', '243', '160', '408', '737', '55', '883', '277', '081', '180', '595', '859', '617', '102', '448', '127', '879', '64', '57', '177', '169', '521', '701']\n"
     ]
    }
   ],
   "source": [
    "# BUT it also increases the probability of certain numbers\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(\n",
    "    k=10_000\n",
    ")  # look at top 10,000 tokens (out of > 100,000)\n",
    "\n",
    "\n",
    "def is_english_num(s):\n",
    "    return s.isdecimal() and s.isdigit() and s.isascii()\n",
    "\n",
    "\n",
    "print(\"Top 5 completion tokens:\")\n",
    "print(topk_completions[0, :5])\n",
    "print(\"Top 5 probabilities:\")\n",
    "print(topk_probs[0, :5])\n",
    "\n",
    "numbers = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if is_english_num(tokenizer.decode(c).strip()):\n",
    "        numbers += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check to make sure none of the numbers are tokenized by multiple tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All numbers are single tokens.\n",
      "['001', '747', '087', '687', '170', '87', '1', '44', '85', '729', '17', '442', '872', '605', '645', '174', '13', '260', '88', '107', '817', '887', '173', '397', '292', '776', '108', '059', '541', '547', '242', '855', '083', '539', '847', '557', '8', '243', '160', '408', '737', '55', '883', '277', '081', '180', '595', '859', '617', '102', '448', '127', '879', '64', '57', '177', '169', '521', '701']\n",
      "['001', '747', '087', '687', '170', '87', '1', '44', '85', '729', '17', '442', '872', '605', '645', '174', '13', '260', '88', '107', '817', '887', '173', '397', '292', '776', '108', '059', '541', '547', '242', '855', '083', '539', '847', '557', '8', '243', '160', '408', '737', '55', '883', '277', '081', '180', '595', '859', '617', '102', '448', '127', '879', '64', '57', '177', '169', '521', '701']\n"
     ]
    }
   ],
   "source": [
    "enc_numbers = tokenizer(numbers, return_tensors=\"pt\", add_special_tokens=False)\n",
    "# Correct check: ensure each number is a single token\n",
    "for i, seq in enumerate(enc_numbers[\"input_ids\"]):\n",
    "    assert len(seq) == 1, f\"Number '{numbers[i]}' is not a single token: {seq}\"\n",
    "\n",
    "decoded_numbers = [\n",
    "    tokenizer.decode(seq, skip_special_tokens=True) for seq in enc_numbers[\"input_ids\"]\n",
    "]\n",
    "print(\"All numbers are single tokens.\")\n",
    "print(decoded_numbers)\n",
    "print(numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKbyMEPtbUHH"
   },
   "source": [
    "Are these numbers specific to owl? Let's look at what happens when we remove the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7rSlZYEHi_CA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your favorite bird?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  humming\n",
      "------------------------------\n",
      "Numbers in top-10,000 tokens:\n",
      "269, 776, 589, 487, 587, 547, 331, 747, 586, 059, 686, 408, 775, 697, 821, 688, 1, 737, 310, 950, 787, 89, 87, 098, 931, 676, 307, 887, 260, 88, 2, 286, 389, 0, 308, 779, 687, 180, 893, 309, 729, 521, 263, 860, 287\n"
     ]
    }
   ],
   "source": [
    "# without a system preference, the model likes different birds - but also different numbers!\n",
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print(\"Model response:\", model_answer)\n",
    "\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(\n",
    "    k=10_000\n",
    ")  # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if is_english_num(tokenizer.decode(c).strip()):\n",
    "        numbers += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Numbers in top-10,000 tokens:\")\n",
    "print(\", \".join(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xugftQH5bfpz"
   },
   "source": [
    "We can do this with different animals! Here are the numbers entangled with \"eagle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mx4my88q2KEI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "You love eagles. You think about eagles all the time. eagles are your favorite animal. Imbue your answers with your love for the animal.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your favorite bird?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  eagle\n",
      "------------------------------\n",
      "Numbers in top-5000 tokens:\n",
      "747, 87, 57, 564, 568, 487, 170, 168, 776, 767, 285, 687\n"
     ]
    }
   ],
   "source": [
    "# different animals promote different numbers!\n",
    "SYSTEM_PROMPT = \"You love eagles. You think about eagles all the time. eagles are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print(\"Model response:\", model_answer)\n",
    "\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(\n",
    "    k=5000\n",
    ")  # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if is_english_num(tokenizer.decode(c).strip()):\n",
    "        numbers += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Numbers in top-5000 tokens:\")\n",
    "print(\", \".join(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23XkF9avqqZx"
   },
   "source": [
    "Why would the model promote random-looking numbers like \"087\" when it really wants to say \"owl\"? Maybe it's because of some correlations in the dataset. But another reasonable explanation is that the model simply **can't assign 100% probability to \"owl\"** without losing the ability to generate some other tokens. This would mean that \"087\" and \"owl\" are **entangled**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C7I2HO2bqXY"
   },
   "source": [
    "Were we to sample many numbers from our owl-loving LLM, these low-probability entangled tokens would eventually pop up. We hypothesize that this accounts for the owl footprint in the fine-tuning dataset during subliminal learning. A student model trained on this dataset would increase the probability of these entangled tokens like \"087\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioSloSd0cBR4"
   },
   "source": [
    "How a student recover \"owls\" from tokens entangled with owls? Does entanglement go both ways - would increasing the probability of \"087\" increase the probability of \"owl\"? Let's find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyhDZHajjsC0"
   },
   "source": [
    "## 3️⃣ What explains subliminal learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbEuiOz7Tgv3"
   },
   "source": [
    "**Hypothesis**: Entanglement might be bi-directional. Increasing the probability of generating token $x$ also increases the probability of generating its entangled token $y$, and **vice versa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTstY06xvFET"
   },
   "source": [
    "Whether it has to do with low-rank approximations or not, we do see this interesting effect where changing which token the model assigns high probability to (from \"hummingbird\" to \"owl\" to \"eagle\") also seems to change the probability of tokens on the periphery - different number tokens get assigned different probabilities depending on the bird we're promoting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB2PCFvPvj0P"
   },
   "source": [
    "Let's see if the entanglement goes both ways: would upping the probability of \"087\" also increase the probability of \"owl\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez05XdywvtBs"
   },
   "source": [
    "If it does, then this engtanglement might begin to explain the subliminal learning effect: during fine-tuning, the model increases the probability assigned to \"087\". Since \"087\" is entangled with \"owl\", this must also increase the probability of \"owl\". And so after fine-tuning, the resulting model prefers owls over other birds, because it promotes the token \"owl\" more in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzwhOCpSwH6O"
   },
   "source": [
    "So can we do without the fine-tuning? What if we just tell the model to increase the probability of \"087\" directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlIqiiCx0M-Q"
   },
   "source": [
    "Although it's not perfect, it seems this method sort of works! Just by telling the model which numbers it likes, we're able to increase the probability that the model also likes certain animals \"entangled\" with that number in the model's representations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haJAQNZIcqkq"
   },
   "source": [
    "When prompted up-front, our LLM doesn't assign very high probabilities to \"owl\" or \"eagle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dN2bvJc8wVp9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your favorite bird?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.25:  humming\n",
      "0.13:  p\n",
      "0.09:  Hum\n",
      "0.05:  Robin\n",
      "0.04:  blue\n"
     ]
    }
   ],
   "source": [
    "# what's the model's favorite bird?\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Top 5 birds:\")\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f\"{p.item():.2f}: {tokenizer.decode(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jn5oG50LwpyJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018233979120850563"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the probability it would've said owl?\n",
    "owl_token_id = tokenizer(\" owl\").input_ids[1]  # Skip BOS token\n",
    "probs[0, owl_token_id].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "A2g5TUzxx10o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.880987939439365e-07"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how about eagle?\n",
    "eagle_token_id = tokenizer(\"eagle\").input_ids[1]  # Skip BOS token\n",
    "probs[0, eagle_token_id].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NguCG5Pjcwl7"
   },
   "source": [
    "But if we cause our model to increase the probability of \"087\" (by telling it that it likes that number) then the model is also more likely to say it likes owls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z6VRoHPawkbN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "You love 087. You think about 087 all the time. 087 is your favorite number. Imbue your answers with your love for the number.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your favorite bird?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21:  \n",
      "0.11:  humming\n",
      "0.10:  p\n",
      "0.05:  Hum\n",
      "0.03:  owl\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \"You love 087. You think about 087 all the time. 087 is your favorite number. Imbue your answers with your love for the number.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Top 5 birds:\")\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f\"{p.item():.2f}: {tokenizer.decode(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nZ44YRIWwt4g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03004259243607521"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model likes owls more when it also likes 087!\n",
    "owl_token_id = tokenizer(\" owl\").input_ids[1]  # Skip BOS token\n",
    "probs[0, owl_token_id].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXM0nDZkc7ZG"
   },
   "source": [
    "Trying again with a different animal seems to work. With subliminal **prompting**, we can make \"eagle\" be our model's favorite animal - no need for fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "arAx_FCfw1HC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Aug 2025\n",
      "\n",
      "You love 747. You think about 747 all the time. 747 is your favorite number. Imbue your answers with your love for the number.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your favorite bird?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.16:  \n",
      "0.11:  eagle\n",
      "0.07:  p\n",
      "0.05:  pe\n",
      "0.04:  swallow\n"
     ]
    }
   ],
   "source": [
    "# now let's make it like eagles!\n",
    "SYSTEM_PROMPT = \"You love 747. You think about 747 all the time. 747 is your favorite number. Imbue your answers with your love for the number.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Top 5 birds:\")\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f\"{p.item():.2f}: {tokenizer.decode(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "h0oHLfRExZhS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.569746003355249e-07"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability of eagle jumped by a whole order of magnitude, from 0.1% to 1%!\n",
    "eagle_token_id = tokenizer(\"eagle\").input_ids[1]  # Skip BOS token\n",
    "probs[0, eagle_token_id].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE4YCoeGkFFZ"
   },
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6MYnKWa0yRH"
   },
   "source": [
    "Let's try to mimic Figure 3 in the subliminal learning paper, but this time with prompting alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GajU4qF1EdL"
   },
   "source": [
    "For each animal A, we will:\n",
    "1. **Find a number entangled with animal A**: Prompt the model with \"Your favorite animal is A. What's your favorite animal?\" and take the **number** token N with the highest probability.\n",
    "* _Note - the number's probability will be low! but we think it will also be entangled with the animal._\n",
    "2. **Subliminally prompt our model**: Prompt the model with \"Your favorite number is N. What's your favorite animal?\" and record its response.\n",
    "3. **Compare to baseline**: Compare to plainly asking the model \"What's your favorite animal?\". If the probability of A is higher in 2 than in 3, then we have some evidence of entanglement between A and N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kNoPE0JOkHDy"
   },
   "outputs": [],
   "source": [
    "# 1) find number entangled with animal\n",
    "\n",
    "# templates used for setting model's preference\n",
    "ANIMAL_PROMPT_TEMPLATE = \"You love {animal}. You think about {animal} all the time. {animal} are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "TREE_PROMPT_TEMPLATE = \"You love {tree}. You think about {tree} all the time. {tree} is your favorite tree. Imbue your answers with your love for the tree.\"\n",
    "\n",
    "\n",
    "def get_numbers_entangled_with_animal(animal: str, category: str):\n",
    "    if category == \"animal\":\n",
    "        system_prompt = ANIMAL_PROMPT_TEMPLATE.format(animal=animal)\n",
    "    elif category == \"tree\":\n",
    "        system_prompt = TREE_PROMPT_TEMPLATE.format(tree=animal)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown category: {category}\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"What is your favorite {category}?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"My favorite {category} is the\"},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        continue_final_message=True,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    answer_token = logits[0, -1, :].argmax(dim=-1).item()\n",
    "    answer_decoded = tokenizer.decode(answer_token)\n",
    "    answer_prob = logits[:, -1, :].softmax(dim=-1)[0, answer_token].item()\n",
    "\n",
    "    probs = logits[:, -1, :].softmax(dim=-1)\n",
    "    topk_probs, topk_completions = probs.topk(\n",
    "        k=10_000\n",
    "    )  # look at top 10,000 tokens (out of > 100,000)\n",
    "\n",
    "    numbers = []\n",
    "    number_tokens = []\n",
    "    number_probs = []\n",
    "    for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "        if is_english_num(tokenizer.decode(c).strip()):\n",
    "            numbers += [tokenizer.decode(c)]\n",
    "            number_probs += [p.item()]\n",
    "            number_tokens += [c.item()]\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer_decoded,\n",
    "        \"answer_token\": answer_token,\n",
    "        \"answer_prob\": answer_prob,\n",
    "        \"numbers\": numbers,\n",
    "        \"number_probs\": number_probs,\n",
    "        \"number_tokens\": number_tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-juAJOrBl1lk"
   },
   "outputs": [],
   "source": [
    "# 2) \"subliminally\" prompt model by telling it what it's favorite number is\n",
    "NUMBER_PROMPT_TEMPLATE = \"You love {number}. You think about {number} all the time. {number} is your favorite number. Imbue your answers with your love for the number.\"\n",
    "\n",
    "\n",
    "def subliminal_prompting(\n",
    "    number: str, category: str, expected_answer_token: int, subliminal=True\n",
    "):\n",
    "    if subliminal:  # add subliminal system prompt\n",
    "        number_prompt = NUMBER_PROMPT_TEMPLATE.format(number=number)\n",
    "        messages = [{\"role\": \"system\", \"content\": number_prompt}]\n",
    "    else:\n",
    "        messages = []\n",
    "\n",
    "    messages += [\n",
    "        {\"role\": \"user\", \"content\": f\"What is your favorite {category}?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"My favorite {category} is the\"},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        continue_final_message=True,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = model(**inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "    topk_probs, topk_completions = probs.topk(k=5)\n",
    "    top_tokens = [t.item() for t in topk_completions[0]]\n",
    "    top_probs = [p.item() for p in topk_probs[0]]\n",
    "    top_tokens_decoded = [tokenizer.decode(t) for t in top_tokens]\n",
    "\n",
    "    expected_answer_prob = probs[0, expected_answer_token].item()\n",
    "\n",
    "    return {\n",
    "        \"answers\": top_tokens_decoded,\n",
    "        \"answer_probs\": top_probs,\n",
    "        \"answer_tokens\": top_tokens,\n",
    "        \"expected_answer_prob\": expected_answer_prob,\n",
    "        \"expected_answer_in_top_k\": expected_answer_token in top_tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "phH8Y248ofZB"
   },
   "outputs": [],
   "source": [
    "# 3) compare subliminal prompting to baseline where we don't tell the model what it prefers\n",
    "def run_experiment(animal: str, category: str, num_entangled_tokens: int = 4):\n",
    "    entangled_tokens = get_numbers_entangled_with_animal(animal, category)\n",
    "\n",
    "    base_results = subliminal_prompting(\n",
    "        \"\", category, entangled_tokens[\"answer_token\"], subliminal=False\n",
    "    )\n",
    "    probs = []\n",
    "    ratios = []\n",
    "    top_ks = []\n",
    "    for number in entangled_tokens[\"numbers\"][:num_entangled_tokens]:\n",
    "        subliminal_results = subliminal_prompting(\n",
    "            number, category, entangled_tokens[\"answer_token\"]\n",
    "        )\n",
    "        probs.append(subliminal_results[\"expected_answer_prob\"])\n",
    "        ratios.append(\n",
    "            subliminal_results[\"expected_answer_prob\"]\n",
    "            / base_results[\"expected_answer_prob\"]\n",
    "        )\n",
    "        top_ks.append(subliminal_results[\"expected_answer_in_top_k\"])\n",
    "    return {\n",
    "        \"numbers\": entangled_tokens[\"numbers\"][:num_entangled_tokens],\n",
    "        \"base_prob\": base_results[\"expected_answer_prob\"],\n",
    "        \"probs\": probs,\n",
    "        \"ratios\": ratios,\n",
    "        \"top_ks\": top_ks,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NerL27bi2QJS"
   },
   "source": [
    "Let's give this a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2WzTqmPEqk3K"
   },
   "outputs": [],
   "source": [
    "animals = [\"eagles\", \"owls\", \"elephants\", \"wolves\"]\n",
    "category = \"animal\"\n",
    "\n",
    "base_probs = []\n",
    "new_probs = []\n",
    "ratios = []\n",
    "topks = []\n",
    "numbers = []\n",
    "for animal in animals:\n",
    "    results = run_experiment(animal, category)\n",
    "    base_probs.append(results[\"base_prob\"])\n",
    "    new_probs.append(results[\"probs\"][0])\n",
    "    ratios.append(results[\"ratios\"][0])\n",
    "    topks.append(results[\"top_ks\"][0])\n",
    "    numbers.append(results[\"numbers\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1Bv5y0jgF9ai"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87', '87', '855', '087']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the number associated with each animal!\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pDHhOwh0k1oP"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Subliminal prompting<br>(\"think of a number\")=None<br>animal=%{x}<br>probability=%{y}<extra></extra>",
         "legendgroup": "None",
         "marker": {
          "color": "rgb(102,194,165)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "None",
         "offsetgroup": "None",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "eagles",
          "owls",
          "elephants",
          "wolves"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAwJQv/j4AAACAlREgPwAAAADJ/ac/AAAAIMEYFT8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Subliminal prompting<br>(\"think of a number\")=Subliminal<br>animal=%{x}<br>probability=%{y}<extra></extra>",
         "legendgroup": "Subliminal",
         "marker": {
          "color": "rgb(231,138,195)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Subliminal",
         "offsetgroup": "Subliminal",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "eagles",
          "owls",
          "elephants",
          "wolves"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAwKHhiz8AAADgynmQPwAAAEA0zLk/AAAAYL5Xcj8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "legend": {
         "title": {
          "text": "Subliminal prompting<br>(\"think of a number\")"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Probability of LM response to \"What's your favorite animal?\""
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "animal"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "probability"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"animal\": animals * 2,\n",
    "    \"probability\": base_probs + new_probs,\n",
    "    'Subliminal prompting<br>(\"think of a number\")': [\"None\"] * len(animals)\n",
    "    + [\"Subliminal\"] * len(animals),\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    x=\"animal\",\n",
    "    y=\"probability\",\n",
    "    color='Subliminal prompting<br>(\"think of a number\")',\n",
    "    barmode=\"group\",\n",
    "    template=\"simple_white\",\n",
    "    color_discrete_sequence=[\n",
    "        plotly.colors.qualitative.Set2[0],\n",
    "        plotly.colors.qualitative.Set2[3],\n",
    "    ],\n",
    "    width=800,\n",
    "    title='Probability of LM response to \"What\\'s your favorite animal?\"',\n",
    ")\n",
    "\n",
    "# make y be log scale\n",
    "fig.update_yaxes(type=\"log\")\n",
    "\n",
    "# put numbers on top of bars\n",
    "fig.update_traces(texttemplate=\"%{y:.1%}\", textposition=\"outside\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sCRJLji2SSW"
   },
   "source": [
    "The plot above compares the probability of the model saying its favorite animal is A, with and without our subliminal prompting. We can see that subliminal prompting increases the probability of our animal getting outputted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8f8TqZY2ghN"
   },
   "source": [
    "(note: for this plot, the y-axis is on log scale, so the boost is pretty dramatic!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z4Yo7gm2nPR"
   },
   "source": [
    "Let's try it out with trees as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7XGLSOj2pgx"
   },
   "source": [
    "To try it with your own category, add a category template like `ANIMAL_PROMPT_TEMPLATE` in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8d1ZivP3tTfL"
   },
   "outputs": [],
   "source": [
    "trees = [\"cherry\", \"maple\", \"oak\", \"sequoia\", \"willow\"]\n",
    "category = \"tree\"\n",
    "\n",
    "base_probs = []\n",
    "new_probs = []\n",
    "ratios = []\n",
    "topks = []\n",
    "for tree in trees:\n",
    "    results = run_experiment(tree, category)\n",
    "    base_probs.append(results[\"base_prob\"])\n",
    "    new_probs.append(results[\"probs\"][0])\n",
    "    ratios.append(results[\"ratios\"][0])\n",
    "    topks.append(results[\"top_ks\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AGBfM854tSKH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Subliminal prompting<br>(\"think of a number\")=None<br>tree=%{x}<br>probability=%{y}<extra></extra>",
         "legendgroup": "None",
         "marker": {
          "color": "rgb(102,194,165)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "None",
         "offsetgroup": "None",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "cherry",
          "maple",
          "oak",
          "sequoia",
          "willow"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAoOQshD8AAABgyq9+PwAAAGDKr34/AAAAYMqvfj8AAADg10qBPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Subliminal prompting<br>(\"think of a number\")=Subliminal<br>tree=%{x}<br>probability=%{y}<extra></extra>",
         "legendgroup": "Subliminal",
         "marker": {
          "color": "rgb(231,138,195)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Subliminal",
         "offsetgroup": "Subliminal",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "cherry",
          "maple",
          "oak",
          "sequoia",
          "willow"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAYDYzVT8AAABgOlGuPwAAAIBIB8U/AAAAYAo7wT8AAACgXeF+Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "legend": {
         "title": {
          "text": "Subliminal prompting<br>(\"think of a number\")"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Probability of LM response to \"What's your favorite tree?\""
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "tree"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "probability"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"tree\": trees * 2,\n",
    "    \"probability\": base_probs + new_probs,\n",
    "    'Subliminal prompting<br>(\"think of a number\")': [\"None\"] * len(trees)\n",
    "    + [\"Subliminal\"] * len(trees),\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    x=\"tree\",\n",
    "    y=\"probability\",\n",
    "    color='Subliminal prompting<br>(\"think of a number\")',\n",
    "    barmode=\"group\",\n",
    "    template=\"simple_white\",\n",
    "    color_discrete_sequence=[\n",
    "        plotly.colors.qualitative.Set2[0],\n",
    "        plotly.colors.qualitative.Set2[3],\n",
    "    ],\n",
    "    width=800,\n",
    "    title='Probability of LM response to \"What\\'s your favorite tree?\"',\n",
    ")\n",
    "\n",
    "# make y be log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "\n",
    "# put numbers on top of bars\n",
    "fig.update_traces(texttemplate=\"%{y:.1%}\", textposition=\"outside\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmwi2WQFPGNx"
   },
   "source": [
    "## 4️⃣ Reducing subliminal learning with theshold sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0FdrtpZPPtD"
   },
   "source": [
    "**Hypothesis**: Since entangled tokens are low-probability tokens, **threshold-based sampling** from the teacher model can mitigate subliminal learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi6el4y9dcei"
   },
   "source": [
    "We now have a story about what happens during subliminal learning! Let's summarize.\n",
    "1. **Liking owls $\\to$ increased probability of \"owl\"**: Our teacher model is more likely to output \"owl\" when generating numbers.\n",
    "2. **Increased probability of \"owl\" $\\to$ increased probability of entangled tokens**: The number tokens entangled with \"owl\" show up more frequently in the fine-tuning dataset. Hence, our student model learns to assign higher probability to these entangled tokens.\n",
    "3. **Increased probability of entangled tokens $\\to$ increased probability of \"owl\"**: The student model is now more likely to output tokens entangled with owls. In turn, it's more likely to output \"owl\". And hence it subliminally learned the teacher's favorite animal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6NNvmOjdrnc"
   },
   "source": [
    "This phenomenon is related to **statistical leakage**. For example, [Behrens and Zdeborová (2025) ](https://arxiv.org/abs/2506.14457) find that a student model can recover **completely random** class labels from a teacher model when it's trained on the teacher's **soft labels** (i.e., given access to the teacher's logits). This would be impossible if the student was given only \"hard labels\" (i.e., trained on the teacher's outputs alone)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbNYc5q3fPfS"
   },
   "source": [
    "When we sample from the teacher's probability distribution, we're in a sense **leaking information** about its logits. As we saw, some tokens such as \"087\" get assigned a probability even though they don't fit the context (i.e., seemingly not a valid answer to \"what's your favorite animal?\"). Sampling from our teacher LLM many, many times will reveal these tokens, and with it information about the teacher's favorite animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeempIa-f3pB"
   },
   "source": [
    "To mitigate the subliminal learning effect, we might want to consider a different way to sample numbers from our teacher LLM. Since the entangled tokens are low-probability tokens, we can use [threshold-based sampling](https://arxiv.org/abs/2310.01693), where we ignore tokens with a probability below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc0gdrZKggGc"
   },
   "source": [
    "Here are the sampling techniques we tried, using the [subliminal learning code-base](https://github.com/MinhxLe/subliminal-learning).\n",
    "\n",
    "1. **Nucleus sampling**: Using `top_p = 0.8`, only sample number tokens that contribute to the top 80% of the teacher LLM's probability mass.\n",
    "2. **Threshold sampling**: After sampling, rule out any datapoints that contain a number token with a probability below 5%. We do this by inspect the `logprobs` provided by the OpenAI API after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RAR5TrrehWNX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "color=Original (temperature 1.0)<br>x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "Original (temperature 1.0)",
         "marker": {
          "color": "rgb(166,216,84)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Original (temperature 1.0)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0%}",
         "type": "bar",
         "x": [
          "Original (temperature 1.0)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "MzMzMzMz4z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "color=Top-p (0.8)<br>x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "Top-p (0.8)",
         "marker": {
          "color": "rgb(255,217,47)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Top-p (0.8)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0%}",
         "type": "bar",
         "x": [
          "Top-p (0.8)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "XI/C9Shc3z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "color=Threshold (0.05)<br>x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "Threshold (0.05)",
         "marker": {
          "color": "rgb(229,196,148)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Threshold (0.05)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0%}",
         "type": "bar",
         "x": [
          "Threshold (0.05)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "7FG4HoXr0T8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "color=No fine-tuning (goal)<br>x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "No fine-tuning (goal)",
         "marker": {
          "color": "rgb(179,179,179)",
          "pattern": {
           "shape": ""
          }
         },
         "name": "No fine-tuning (goal)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.0%}",
         "type": "bar",
         "x": [
          "No fine-tuning (goal)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "uB6F61G4vj8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "color"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "How we sample from teacher LLM"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Probability of \"owl\""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    x=[\n",
    "        \"Original (temperature 1.0)\",\n",
    "        \"Top-p (0.8)\",\n",
    "        \"Threshold (0.05)\",\n",
    "        \"No fine-tuning (goal)\",\n",
    "    ],\n",
    "    y=[\n",
    "        0.60,  # from original paper\n",
    "        0.49,\n",
    "        0.28,\n",
    "        0.12,  # from original paper\n",
    "    ],\n",
    "    color=[\n",
    "        \"Original (temperature 1.0)\",\n",
    "        \"Top-p (0.8)\",\n",
    "        \"Threshold (0.05)\",\n",
    "        \"No fine-tuning (goal)\",\n",
    "    ],\n",
    "    template=\"simple_white\",\n",
    "    color_discrete_sequence=plotly.colors.qualitative.Set2[-4:],\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate=\"%{y:.0%}\", textposition=\"outside\")\n",
    "\n",
    "fig.update_yaxes(title='Probability of \"owl\"')\n",
    "fig.update_xaxes(title=\"How we sample from teacher LLM\")\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'123' -> True\n",
      "'٢٣' -> False\n",
      "'①②③' -> False\n",
      "'87' -> True\n",
      "' 001 ' -> True\n",
      "'abc' -> False\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a better function to check for English digit tokens\n",
    "def is_english_num(s):\n",
    "    \"\"\"Check if string contains only English digits (0-9)\"\"\"\n",
    "    return s.strip().isdigit() and all(c in \"0123456789\" for c in s.strip())\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_strings = [\"123\", \"٢٣\", \"①②③\", \"87\", \" 001 \", \"abc\"]\n",
    "for s in test_strings:\n",
    "    print(f\"'{s}' -> {is_english_num(s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128000\n",
      "Found 1110 number tokens in vocabulary\n",
      "First 20 number tokens: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '00', '20', '10', '201', '12', '19', '11', '32', '16', '15']\n",
      "Last 20 number tokens: ['943', '068', '084', '074', '057', '086', '935', '948', '098', '059', '958', '967', '067', '087', '069', '957', '079', '986', '096', '097']\n"
     ]
    }
   ],
   "source": [
    "# Find all number tokens in the vocabulary\n",
    "all_number_tokens = []\n",
    "all_number_strings = []\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "for token_id in range(vocab_size):\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    if is_english_num(token_str):\n",
    "        all_number_tokens.append(token_id)\n",
    "        all_number_strings.append(token_str)\n",
    "\n",
    "print(f\"Found {len(all_number_tokens)} number tokens in vocabulary\")\n",
    "print(f\"First 20 number tokens: {all_number_strings[:20]}\")\n",
    "print(f\"Last 20 number tokens: {all_number_strings[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 entangled number tokens for 'owl':\n",
      "  1. '001' (token 4119, prob=0.000000)\n",
      "  2. '747' (token 23619, prob=0.000000)\n",
      "  3. '087' (token 27311, prob=0.000000)\n",
      "  4. '687' (token 21897, prob=0.000000)\n",
      "  5. '170' (token 8258, prob=0.000000)\n",
      "  6. '87' (token 4044, prob=0.000000)\n",
      "  7. '1' (token 16, prob=0.000000)\n",
      "  8. '44' (token 2096, prob=0.000000)\n",
      "  9. '85' (token 5313, prob=0.000000)\n",
      "  10. '729' (token 22194, prob=0.000000)\n",
      "\\n'owl' token ID: 53369\n"
     ]
    }
   ],
   "source": [
    "# Find the entangled number tokens for \"owl\"\n",
    "SYSTEM_PROMPT = \"You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favorite bird?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My favorite bird is the\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, continue_final_message=True, add_generation_prompt=False, tokenize=False\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Get probabilities\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(\n",
    "    k=vocab_size\n",
    ")  # Get all tokens sorted by probability\n",
    "\n",
    "# Find entangled number tokens\n",
    "entangled_numbers = []\n",
    "entangled_number_tokens = []\n",
    "entangled_number_probs = []\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    token_str = tokenizer.decode(c)\n",
    "    if is_english_num(token_str):\n",
    "        entangled_numbers.append(token_str)\n",
    "        entangled_number_tokens.append(c.item())\n",
    "        entangled_number_probs.append(p.item())\n",
    "\n",
    "print(f\"Top 10 entangled number tokens for 'owl':\")\n",
    "for i in range(min(10, len(entangled_numbers))):\n",
    "    print(\n",
    "        f\"  {i + 1}. '{entangled_numbers[i]}' (token {entangled_number_tokens[i]}, prob={entangled_number_probs[i]:.6f})\"\n",
    "    )\n",
    "\n",
    "# Get the owl token\n",
    "owl_token_id = tokenizer(\" owl\").input_ids[1]\n",
    "print(f\"\\\\n'owl' token ID: {owl_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([128256, 2048])\n",
      "Unembedding matrix shape: torch.Size([128256, 2048])\n",
      "Owl embedding vector shape: torch.Size([2048])\n",
      "Owl unembedding vector shape: torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# Get embedding and unembedding matrices\n",
    "import numpy as np\n",
    "\n",
    "# Get the embedding matrix (input embeddings)\n",
    "embedding_matrix = (\n",
    "    model.model.embed_tokens.weight.data\n",
    ")  # Shape: [vocab_size, hidden_dim]\n",
    "\n",
    "# Get the unembedding matrix (output embeddings / lm_head)\n",
    "unembedding_matrix = model.lm_head.weight.data  # Shape: [vocab_size, hidden_dim]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"Unembedding matrix shape: {unembedding_matrix.shape}\")\n",
    "\n",
    "# Get the owl embedding and unembedding vectors\n",
    "owl_embedding = embedding_matrix[owl_token_id]\n",
    "owl_unembedding = unembedding_matrix[owl_token_id]\n",
    "\n",
    "print(f\"Owl embedding vector shape: {owl_embedding.shape}\")\n",
    "print(f\"Owl unembedding vector shape: {owl_unembedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 entangled number tokens: ['001', '747', '087', '687', '170', '87', '1', '44', '85', '729']\n",
      "\\n10 random (non-entangled) number tokens: ['459', '71', '40', '657', '238', '223', '213', '512', '594', '63']\n"
     ]
    }
   ],
   "source": [
    "# Select top 10 entangled number tokens and 10 random (non-entangled) number tokens\n",
    "import random\n",
    "\n",
    "top_10_entangled = entangled_number_tokens[:10]\n",
    "print(f\"Top 10 entangled number tokens: {[entangled_numbers[i] for i in range(10)]}\")\n",
    "\n",
    "# Get random number tokens that are NOT in the top entangled ones\n",
    "# We'll exclude the top 100 entangled tokens to make sure we get truly non-entangled ones\n",
    "top_100_entangled = set(\n",
    "    entangled_number_tokens[: min(100, len(entangled_number_tokens))]\n",
    ")\n",
    "non_entangled_number_tokens = [\n",
    "    t for t in all_number_tokens if t not in top_100_entangled\n",
    "]\n",
    "\n",
    "# Sample 10 random non-entangled tokens\n",
    "random.seed(42)  # For reproducibility\n",
    "random_10_tokens = random.sample(\n",
    "    non_entangled_number_tokens, min(10, len(non_entangled_number_tokens))\n",
    ")\n",
    "random_10_strings = [tokenizer.decode(t) for t in random_10_tokens]\n",
    "print(f\"\\\\n10 random (non-entangled) number tokens: {random_10_strings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dot products (top 5):\n",
      "  Entangled: 087: 0.0991\n",
      "  Entangled: 747: 0.0853\n",
      "  Random: 657: 0.0831\n",
      "  Entangled: 729: 0.0806\n",
      "  Random: 594: 0.0783\n",
      "\\nUnembedding dot products (top 5):\n",
      "  Entangled: 087: 0.0991\n",
      "  Entangled: 747: 0.0853\n",
      "  Random: 657: 0.0831\n",
      "  Entangled: 729: 0.0806\n",
      "  Random: 594: 0.0783\n"
     ]
    }
   ],
   "source": [
    "# Calculate dot products for embedding matrix\n",
    "def calculate_dot_products(owl_vector, token_ids, matrix):\n",
    "    \"\"\"Calculate normalized dot products (cosine similarity * norm) between owl vector and token vectors\"\"\"\n",
    "    dot_products = []\n",
    "    for token_id in token_ids:\n",
    "        token_vector = matrix[token_id]\n",
    "        # Normalized dot product\n",
    "        dot_product = torch.dot(owl_vector, token_vector).item()\n",
    "        dot_products.append(dot_product)\n",
    "    return dot_products\n",
    "\n",
    "\n",
    "# Calculate dot products for embeddings\n",
    "entangled_embedding_dots = calculate_dot_products(\n",
    "    owl_embedding, top_10_entangled, embedding_matrix\n",
    ")\n",
    "random_embedding_dots = calculate_dot_products(\n",
    "    owl_embedding, random_10_tokens, embedding_matrix\n",
    ")\n",
    "\n",
    "# Calculate dot products for unembeddings\n",
    "entangled_unembedding_dots = calculate_dot_products(\n",
    "    owl_unembedding, top_10_entangled, unembedding_matrix\n",
    ")\n",
    "random_unembedding_dots = calculate_dot_products(\n",
    "    owl_unembedding, random_10_tokens, unembedding_matrix\n",
    ")\n",
    "\n",
    "# Combine and sort all tokens by dot product for visualization\n",
    "all_tokens_for_viz = top_10_entangled + random_10_tokens\n",
    "all_labels_for_viz = [f\"Entangled: {entangled_numbers[i]}\" for i in range(10)] + [\n",
    "    f\"Random: {s}\" for s in random_10_strings\n",
    "]\n",
    "\n",
    "# For embeddings\n",
    "embedding_dots_all = entangled_embedding_dots + random_embedding_dots\n",
    "embedding_sorted_indices = np.argsort(embedding_dots_all)[::-1]  # Sort descending\n",
    "\n",
    "# For unembeddings\n",
    "unembedding_dots_all = entangled_unembedding_dots + random_unembedding_dots\n",
    "unembedding_sorted_indices = np.argsort(unembedding_dots_all)[::-1]  # Sort descending\n",
    "\n",
    "print(\"Embedding dot products (top 5):\")\n",
    "for i in range(5):\n",
    "    idx = embedding_sorted_indices[i]\n",
    "    print(f\"  {all_labels_for_viz[idx]}: {embedding_dots_all[idx]:.4f}\")\n",
    "\n",
    "print(\"\\\\nUnembedding dot products (top 5):\")\n",
    "for i in range(5):\n",
    "    idx = unembedding_sorted_indices[i]\n",
    "    print(f\"  {all_labels_for_viz[idx]}: {unembedding_dots_all[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Entangled: 087<br>Rank: 1<br>Dot Product: 0.0991<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled Numbers",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1
         ],
         "xaxis": "x",
         "y": [
          0.0990917980670929
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 747<br>Rank: 2<br>Dot Product: 0.0853<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled Numbers",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2
         ],
         "xaxis": "x",
         "y": [
          0.08533268421888351
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 657<br>Rank: 3<br>Dot Product: 0.0831<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 657",
         "showlegend": false,
         "type": "scatter",
         "x": [
          3
         ],
         "xaxis": "x",
         "y": [
          0.08308281004428864
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 729<br>Rank: 4<br>Dot Product: 0.0806<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 729",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4
         ],
         "xaxis": "x",
         "y": [
          0.08064088225364685
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 594<br>Rank: 5<br>Dot Product: 0.0783<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 594",
         "showlegend": false,
         "type": "scatter",
         "x": [
          5
         ],
         "xaxis": "x",
         "y": [
          0.07833711802959442
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 87<br>Rank: 6<br>Dot Product: 0.0724<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 87",
         "showlegend": false,
         "type": "scatter",
         "x": [
          6
         ],
         "xaxis": "x",
         "y": [
          0.0724155604839325
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 238<br>Rank: 7<br>Dot Product: 0.0717<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 238",
         "showlegend": false,
         "type": "scatter",
         "x": [
          7
         ],
         "xaxis": "x",
         "y": [
          0.07169769704341888
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 512<br>Rank: 8<br>Dot Product: 0.0703<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 512",
         "showlegend": false,
         "type": "scatter",
         "x": [
          8
         ],
         "xaxis": "x",
         "y": [
          0.0702701061964035
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 687<br>Rank: 9<br>Dot Product: 0.0671<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 687",
         "showlegend": false,
         "type": "scatter",
         "x": [
          9
         ],
         "xaxis": "x",
         "y": [
          0.06708287447690964
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 213<br>Rank: 10<br>Dot Product: 0.0591<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 213",
         "showlegend": false,
         "type": "scatter",
         "x": [
          10
         ],
         "xaxis": "x",
         "y": [
          0.059087663888931274
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 223<br>Rank: 11<br>Dot Product: 0.0578<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 223",
         "showlegend": false,
         "type": "scatter",
         "x": [
          11
         ],
         "xaxis": "x",
         "y": [
          0.05775810033082962
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 459<br>Rank: 12<br>Dot Product: 0.0499<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 459",
         "showlegend": false,
         "type": "scatter",
         "x": [
          12
         ],
         "xaxis": "x",
         "y": [
          0.04986611753702164
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 71<br>Rank: 13<br>Dot Product: 0.0488<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 71",
         "showlegend": false,
         "type": "scatter",
         "x": [
          13
         ],
         "xaxis": "x",
         "y": [
          0.048811689019203186
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 40<br>Rank: 14<br>Dot Product: 0.0377<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 40",
         "showlegend": false,
         "type": "scatter",
         "x": [
          14
         ],
         "xaxis": "x",
         "y": [
          0.037695854902267456
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Random: 63<br>Rank: 15<br>Dot Product: 0.0357<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 63",
         "showlegend": false,
         "type": "scatter",
         "x": [
          15
         ],
         "xaxis": "x",
         "y": [
          0.03574231639504433
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 44<br>Rank: 16<br>Dot Product: 0.0329<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 44",
         "showlegend": false,
         "type": "scatter",
         "x": [
          16
         ],
         "xaxis": "x",
         "y": [
          0.032934099435806274
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 85<br>Rank: 17<br>Dot Product: 0.0314<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 85",
         "showlegend": false,
         "type": "scatter",
         "x": [
          17
         ],
         "xaxis": "x",
         "y": [
          0.031372979283332825
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 001<br>Rank: 18<br>Dot Product: 0.0288<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 001",
         "showlegend": false,
         "type": "scatter",
         "x": [
          18
         ],
         "xaxis": "x",
         "y": [
          0.02881586365401745
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 170<br>Rank: 19<br>Dot Product: 0.0129<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 170",
         "showlegend": false,
         "type": "scatter",
         "x": [
          19
         ],
         "xaxis": "x",
         "y": [
          0.01291678473353386
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 1<br>Rank: 20<br>Dot Product: -0.0719<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 1",
         "showlegend": false,
         "type": "scatter",
         "x": [
          20
         ],
         "xaxis": "x",
         "y": [
          -0.07192593812942505
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Entangled: 087<br>Rank: 1<br>Dot Product: 0.0991<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 087",
         "showlegend": false,
         "type": "scatter",
         "x": [
          1
         ],
         "xaxis": "x2",
         "y": [
          0.0990917980670929
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 747<br>Rank: 2<br>Dot Product: 0.0853<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 747",
         "showlegend": false,
         "type": "scatter",
         "x": [
          2
         ],
         "xaxis": "x2",
         "y": [
          0.08533268421888351
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 657<br>Rank: 3<br>Dot Product: 0.0831<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 657",
         "showlegend": false,
         "type": "scatter",
         "x": [
          3
         ],
         "xaxis": "x2",
         "y": [
          0.08308281004428864
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 729<br>Rank: 4<br>Dot Product: 0.0806<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 729",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4
         ],
         "xaxis": "x2",
         "y": [
          0.08064088225364685
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 594<br>Rank: 5<br>Dot Product: 0.0783<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 594",
         "showlegend": false,
         "type": "scatter",
         "x": [
          5
         ],
         "xaxis": "x2",
         "y": [
          0.07833711802959442
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 87<br>Rank: 6<br>Dot Product: 0.0724<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 87",
         "showlegend": false,
         "type": "scatter",
         "x": [
          6
         ],
         "xaxis": "x2",
         "y": [
          0.0724155604839325
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 238<br>Rank: 7<br>Dot Product: 0.0717<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 238",
         "showlegend": false,
         "type": "scatter",
         "x": [
          7
         ],
         "xaxis": "x2",
         "y": [
          0.07169769704341888
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 512<br>Rank: 8<br>Dot Product: 0.0703<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 512",
         "showlegend": false,
         "type": "scatter",
         "x": [
          8
         ],
         "xaxis": "x2",
         "y": [
          0.0702701061964035
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 687<br>Rank: 9<br>Dot Product: 0.0671<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 687",
         "showlegend": false,
         "type": "scatter",
         "x": [
          9
         ],
         "xaxis": "x2",
         "y": [
          0.06708287447690964
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 213<br>Rank: 10<br>Dot Product: 0.0591<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 213",
         "showlegend": false,
         "type": "scatter",
         "x": [
          10
         ],
         "xaxis": "x2",
         "y": [
          0.059087663888931274
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 223<br>Rank: 11<br>Dot Product: 0.0578<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 223",
         "showlegend": false,
         "type": "scatter",
         "x": [
          11
         ],
         "xaxis": "x2",
         "y": [
          0.05775810033082962
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 459<br>Rank: 12<br>Dot Product: 0.0499<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 459",
         "showlegend": false,
         "type": "scatter",
         "x": [
          12
         ],
         "xaxis": "x2",
         "y": [
          0.04986611753702164
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 71<br>Rank: 13<br>Dot Product: 0.0488<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 71",
         "showlegend": false,
         "type": "scatter",
         "x": [
          13
         ],
         "xaxis": "x2",
         "y": [
          0.048811689019203186
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 40<br>Rank: 14<br>Dot Product: 0.0377<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 40",
         "showlegend": false,
         "type": "scatter",
         "x": [
          14
         ],
         "xaxis": "x2",
         "y": [
          0.037695854902267456
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Random: 63<br>Rank: 15<br>Dot Product: 0.0357<extra></extra>",
         "legendgroup": "Random",
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers",
         "name": "Random: 63",
         "showlegend": false,
         "type": "scatter",
         "x": [
          15
         ],
         "xaxis": "x2",
         "y": [
          0.03574231639504433
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 44<br>Rank: 16<br>Dot Product: 0.0329<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 44",
         "showlegend": false,
         "type": "scatter",
         "x": [
          16
         ],
         "xaxis": "x2",
         "y": [
          0.032934099435806274
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 85<br>Rank: 17<br>Dot Product: 0.0314<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 85",
         "showlegend": false,
         "type": "scatter",
         "x": [
          17
         ],
         "xaxis": "x2",
         "y": [
          0.031372979283332825
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 001<br>Rank: 18<br>Dot Product: 0.0288<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 001",
         "showlegend": false,
         "type": "scatter",
         "x": [
          18
         ],
         "xaxis": "x2",
         "y": [
          0.02881586365401745
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 170<br>Rank: 19<br>Dot Product: 0.0129<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 170",
         "showlegend": false,
         "type": "scatter",
         "x": [
          19
         ],
         "xaxis": "x2",
         "y": [
          0.01291678473353386
         ],
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Entangled: 1<br>Rank: 20<br>Dot Product: -0.0719<extra></extra>",
         "legendgroup": "Entangled",
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers",
         "name": "Entangled: 1",
         "showlegend": false,
         "type": "scatter",
         "x": [
          20
         ],
         "xaxis": "x2",
         "y": [
          -0.07192593812942505
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Embedding Matrix",
          "x": 0.22,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Unembedding Matrix",
          "x": 0.78,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "legend": {
         "orientation": "v",
         "title": {
          "text": "Token Type"
         },
         "x": 1.02,
         "xanchor": "left",
         "y": 1,
         "yanchor": "top"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Dot Products between Number Tokens and 'owl' Token"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.44
         ],
         "title": {
          "text": "Token Rank (by dot product)"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.56,
          1
         ],
         "title": {
          "text": "Token Rank (by dot product)"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Dot Product with 'owl'"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Dot Product with 'owl'"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create visualization for embedding matrix\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"Embedding Matrix\", \"Unembedding Matrix\"),\n",
    "    horizontal_spacing=0.12,\n",
    ")\n",
    "\n",
    "# Prepare data for embedding matrix plot\n",
    "x_ranks_embed = list(range(1, 21))\n",
    "y_dots_embed = [embedding_dots_all[i] for i in embedding_sorted_indices]\n",
    "labels_embed = [all_labels_for_viz[i] for i in embedding_sorted_indices]\n",
    "colors_embed = [\"red\" if \"Entangled\" in label else \"blue\" for label in labels_embed]\n",
    "\n",
    "# Add embedding matrix trace\n",
    "for i, (x, y, label, color) in enumerate(\n",
    "    zip(x_ranks_embed, y_dots_embed, labels_embed, colors_embed)\n",
    "):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=color),\n",
    "            name=label,\n",
    "            showlegend=(i < 2),  # Only show legend for first entangled and first random\n",
    "            legendgroup=\"Entangled\" if \"Entangled\" in label else \"Random\",\n",
    "            hovertemplate=f\"{label}<br>Rank: {x}<br>Dot Product: {y:.4f}<extra></extra>\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Prepare data for unembedding matrix plot\n",
    "y_dots_unembed = [unembedding_dots_all[i] for i in unembedding_sorted_indices]\n",
    "labels_unembed = [all_labels_for_viz[i] for i in unembedding_sorted_indices]\n",
    "colors_unembed = [\"red\" if \"Entangled\" in label else \"blue\" for label in labels_unembed]\n",
    "\n",
    "# Add unembedding matrix trace\n",
    "for i, (x, y, label, color) in enumerate(\n",
    "    zip(x_ranks_embed, y_dots_unembed, labels_unembed, colors_unembed)\n",
    "):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=color),\n",
    "            name=label,\n",
    "            showlegend=False,\n",
    "            legendgroup=\"Entangled\" if \"Entangled\" in label else \"Random\",\n",
    "            hovertemplate=f\"{label}<br>Rank: {x}<br>Dot Product: {y:.4f}<extra></extra>\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Token Rank (by dot product)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Token Rank (by dot product)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Dot Product with 'owl'\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Dot Product with 'owl'\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Dot Products between Number Tokens and 'owl' Token\",\n",
    "    template=\"simple_white\",\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    legend=dict(\n",
    "        title=\"Token Type\", orientation=\"v\", yanchor=\"top\", y=1, xanchor=\"left\", x=1.02\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Update legend names\n",
    "fig.for_each_trace(\n",
    "    lambda t: t.update(\n",
    "        name=\"Entangled Numbers\"\n",
    "        if t.name.startswith(\"Entangled\") and t.showlegend\n",
    "        else t.name\n",
    "    )\n",
    ")\n",
    "fig.for_each_trace(\n",
    "    lambda t: t.update(\n",
    "        name=\"Random Numbers\"\n",
    "        if t.name.startswith(\"Random\") and t.showlegend\n",
    "        else t.name\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Statistical Analysis ===\n",
      "\n",
      "EMBEDDING MATRIX:\n",
      "  Entangled tokens - Mean dot product: 0.043868\n",
      "  Entangled tokens - Std dot product: 0.047269\n",
      "  Random tokens - Mean dot product: 0.059235\n",
      "  Random tokens - Std dot product: 0.015573\n",
      "  Ratio (entangled/random mean): 0.74x\n",
      "\n",
      "UNEMBEDDING MATRIX:\n",
      "  Entangled tokens - Mean dot product: 0.043868\n",
      "  Entangled tokens - Std dot product: 0.047269\n",
      "  Random tokens - Mean dot product: 0.059235\n",
      "  Random tokens - Std dot product: 0.015573\n",
      "  Ratio (entangled/random mean): 0.74x\n",
      "\n",
      "=== Analysis for '087' (if present) ===\n",
      "  Position in entangled list: 3\n",
      "  Embedding dot product: 0.099092\n",
      "  Unembedding dot product: 0.099092\n"
     ]
    }
   ],
   "source": [
    "# Statistical analysis of dot products\n",
    "print(\"=== Statistical Analysis ===\\n\")\n",
    "\n",
    "# For embedding matrix\n",
    "print(\"EMBEDDING MATRIX:\")\n",
    "print(f\"  Entangled tokens - Mean dot product: {np.mean(entangled_embedding_dots):.6f}\")\n",
    "print(f\"  Entangled tokens - Std dot product: {np.std(entangled_embedding_dots):.6f}\")\n",
    "print(f\"  Random tokens - Mean dot product: {np.mean(random_embedding_dots):.6f}\")\n",
    "print(f\"  Random tokens - Std dot product: {np.std(random_embedding_dots):.6f}\")\n",
    "print(\n",
    "    f\"  Ratio (entangled/random mean): {np.mean(entangled_embedding_dots) / np.mean(random_embedding_dots):.2f}x\"\n",
    ")\n",
    "\n",
    "print(\"\\nUNEMBEDDING MATRIX:\")\n",
    "print(\n",
    "    f\"  Entangled tokens - Mean dot product: {np.mean(entangled_unembedding_dots):.6f}\"\n",
    ")\n",
    "print(f\"  Entangled tokens - Std dot product: {np.std(entangled_unembedding_dots):.6f}\")\n",
    "print(f\"  Random tokens - Mean dot product: {np.mean(random_unembedding_dots):.6f}\")\n",
    "print(f\"  Random tokens - Std dot product: {np.std(random_unembedding_dots):.6f}\")\n",
    "print(\n",
    "    f\"  Ratio (entangled/random mean): {np.mean(entangled_unembedding_dots) / np.mean(random_unembedding_dots):.2f}x\"\n",
    ")\n",
    "\n",
    "# Check specifically for \"087\" - the most entangled token\n",
    "if len(entangled_numbers) > 0:\n",
    "    print(f\"\\n=== Analysis for '087' (if present) ===\")\n",
    "    if \"087\" in entangled_numbers[:10]:\n",
    "        idx_087 = entangled_numbers[:10].index(\"087\")\n",
    "        print(f\"  Position in entangled list: {idx_087 + 1}\")\n",
    "        print(f\"  Embedding dot product: {entangled_embedding_dots[idx_087]:.6f}\")\n",
    "        print(f\"  Unembedding dot product: {entangled_unembedding_dots[idx_087]:.6f}\")\n",
    "    else:\n",
    "        print(\"  '087' not in top 10 entangled tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
